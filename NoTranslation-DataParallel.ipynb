{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86200,"databundleVersionId":9773555,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install/Upgrade Packages","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T11:58:38.480642Z","iopub.execute_input":"2024-10-15T11:58:38.481336Z","iopub.status.idle":"2024-10-15T11:59:04.515060Z","shell.execute_reply.started":"2024-10-15T11:58:38.481283Z","shell.execute_reply":"2024-10-15T11:59:04.513977Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.45.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nimport pandas as pd\nimport re\nimport csv\nfrom tqdm import tqdm\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T11:59:04.517707Z","iopub.execute_input":"2024-10-15T11:59:04.518248Z","iopub.status.idle":"2024-10-15T11:59:09.408641Z","shell.execute_reply.started":"2024-10-15T11:59:04.518200Z","shell.execute_reply":"2024-10-15T11:59:09.407730Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Variables","metadata":{}},{"cell_type":"code","source":"# In DEBUG mode, infer only on 5 problems\nDEBUG = False \n# Number of candidate solutions to generate\nK = 4","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:59:09.409748Z","iopub.execute_input":"2024-10-15T11:59:09.410203Z","iopub.status.idle":"2024-10-15T11:59:09.414641Z","shell.execute_reply.started":"2024-10-15T11:59:09.410169Z","shell.execute_reply":"2024-10-15T11:59:09.413727Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"markdown","source":"### Use pandas to read CSV","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/dlsprint3/test.csv')\ntest_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:59:09.415957Z","iopub.execute_input":"2024-10-15T11:59:09.416505Z","iopub.status.idle":"2024-10-15T11:59:09.456803Z","shell.execute_reply.started":"2024-10-15T11:59:09.416453Z","shell.execute_reply":"2024-10-15T11:59:09.455879Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    ID                                            Problem\n52  52  একটি পরিবারে 8 (আট) জন সদস্যের জন্য 9 দিনে 6 ক...\n41  41  একটি ড্রাগন প্রতি 10 তম মিনিটে 5 টা গাছ পুড়িয়ে...\n66  66  মাজেদ গণিত নিয়ে নতুন নতুন আবিষ্কার করতে খুব প...\n11  11  $k$-এর সর্বনিম্ন কোন মানের জন্য $\\sqrt{70 \\tim...\n40  40  এমন দুই অংকের সংখ্যা নির্ণয় কর, যে সংখ্যাগুলো ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Problem</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52</th>\n      <td>52</td>\n      <td>একটি পরিবারে 8 (আট) জন সদস্যের জন্য 9 দিনে 6 ক...</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>একটি ড্রাগন প্রতি 10 তম মিনিটে 5 টা গাছ পুড়িয়ে...</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>66</td>\n      <td>মাজেদ গণিত নিয়ে নতুন নতুন আবিষ্কার করতে খুব প...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>$k$-এর সর্বনিম্ন কোন মানের জন্য $\\sqrt{70 \\tim...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>এমন দুই অংকের সংখ্যা নির্ণয় কর, যে সংখ্যাগুলো ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Initalize Model","metadata":{}},{"cell_type":"markdown","source":"#### We are using DeepSeek Math 7b Instruct with 16 bit precision. You can explore other models.","metadata":{}},{"cell_type":"code","source":"model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n\n# Now, apply DataParallel if needed (use it on the quantized model)\nmodel = torch.nn.DataParallel(model)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T11:59:09.458811Z","iopub.execute_input":"2024-10-15T11:59:09.459137Z","iopub.status.idle":"2024-10-15T12:00:17.896678Z","shell.execute_reply.started":"2024-10-15T11:59:09.459104Z","shell.execute_reply":"2024-10-15T12:00:17.895664Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54d41a265104b5388bef2cbf9bade10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abfefcf8facd4644a453f6f2fe4693e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b2b1995082d41d4a2fe8abcbeb8c677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ce8a11f57104226a867f90bbaa488b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2109bfd1a3ac4088a473cb5b72d9c1a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3133487a159249f28796f80a6305d8dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660b61ee1262481b888cd7c7f16e8e97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0cf35f03b249bea92b4af743234280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7e1c48d39748a9b80a6efbde1e29eb"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Configure Model Generation Parameters","metadata":{}},{"cell_type":"code","source":"model.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.max_new_tokens = 1024\nmodel.generation_config.temperature = 0.7\nmodel.generation_config.top_p = 0.7\nmodel.generation_config.do_sample = True\nmodel.generation_config.num_return_sequences = 4\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\n#model.generation_config = GenerationConfig.from_pretrained(model_name)\n#model.generation_config.max_new_tokens = 512  # Reduced max token count\n#model.generation_config.temperature = 0.5  # Lower temperature for deterministic results\n#model.generation_config.top_p = 0.9  # Slightly higher p to include more token options\n#model.generation_config.do_sample = False  # Make the generation deterministic\n#model.generation_config.num_return_sequences = 1  # Only return 1 sequence\n#model.generation_config.pad_token_id = model.generation_config.eos_token_id\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T12:00:23.642695Z","iopub.execute_input":"2024-10-15T12:00:23.643262Z","iopub.status.idle":"2024-10-15T12:00:23.745344Z","shell.execute_reply.started":"2024-10-15T12:00:23.643221Z","shell.execute_reply":"2024-10-15T12:00:23.744332Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"markdown","source":"#### Extract the problem answer from \\boxed{}","metadata":{}},{"cell_type":"code","source":"def extract_answer(result):\n    match = re.search(r'\\\\boxed{(.*?)}', result)\n    if match:\n        boxed_content = match.group(1)\n        digits = ''.join(filter(str.isdigit, boxed_content))\n        if digits:\n            return int(digits)\n    return None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T12:00:27.506868Z","iopub.execute_input":"2024-10-15T12:00:27.507312Z","iopub.status.idle":"2024-10-15T12:00:31.953414Z","shell.execute_reply.started":"2024-10-15T12:00:27.507272Z","shell.execute_reply":"2024-10-15T12:00:31.952324Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Majority vote between candidate answers","metadata":{}},{"cell_type":"code","source":"def majority_answer(answers):\n    answers = [answer for answer in answers if answer is not None]\n\n    if not answers:\n        return None\n    \n    counts = {}\n    for answer in answers:\n        if answer in counts:\n            counts[answer] += 1\n        else:\n            counts[answer] = 1\n\n    max_answer = None\n    max_count = 0\n    \n    for answer, count in counts.items():\n        if count > max_count:\n            max_answer = answer\n            max_count = count\n    \n    return max_answer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T12:00:31.955081Z","iopub.execute_input":"2024-10-15T12:00:31.955416Z","iopub.status.idle":"2024-10-15T12:00:31.965489Z","shell.execute_reply.started":"2024-10-15T12:00:31.955382Z","shell.execute_reply":"2024-10-15T12:00:31.964483Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Generate solution candidates using LLM","metadata":{}},{"cell_type":"code","source":"def predict_answer(problem):\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": f\"Here is a math problem in Bengali.\\n{problem}\\nPlease solve the problem. Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\n        }\n    ]\n\n    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n    \n    outputs = model.module.generate(input_tensor.to(model.module.device), max_new_tokens=1024)  # or a higher number based on your needs\n\n\n    #outputs = model.generate(input_tensor.to(model.device))\n\n    results = [tokenizer.decode(outputs[i][input_tensor.shape[1]:], skip_special_tokens=True) for i in range(len(outputs))]\n    answers = [extract_answer(result) for result in results]\n    \n    return majority_answer(answers)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-15T12:00:31.966678Z","iopub.execute_input":"2024-10-15T12:00:31.967057Z","iopub.status.idle":"2024-10-15T12:00:31.982865Z","shell.execute_reply.started":"2024-10-15T12:00:31.967012Z","shell.execute_reply":"2024-10-15T12:00:31.981550Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission","metadata":{}},{"cell_type":"code","source":"if DEBUG:\n    test_df = test_df[:5]\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:00:33.877546Z","iopub.execute_input":"2024-10-15T12:00:33.878424Z","iopub.status.idle":"2024-10-15T12:00:33.883100Z","shell.execute_reply.started":"2024-10-15T12:00:33.878383Z","shell.execute_reply":"2024-10-15T12:00:33.881931Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"file = open('token1024Parallel-NoTranslation.csv', 'w', encoding='utf-8')\nwriter = csv.writer(file)\nwriter.writerow(['ID', 'Answer'])\n\nfor row in tqdm(test_df.values):\n    id = row[0]\n    problem = row[1]    \n    answer = predict_answer(problem)\n    \n    if DEBUG:\n        print('id: ', id)\n        print('problem: ', problem)\n        print('answer: ', answer)\n    \n    if answer is None:\n        answer = 0\n        \n    writer.writerow([id, answer])\n    \nfile.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T12:00:37.471003Z","iopub.execute_input":"2024-10-15T12:00:37.472008Z","iopub.status.idle":"2024-10-15T12:46:55.790357Z","shell.execute_reply.started":"2024-10-15T12:00:37.471965Z","shell.execute_reply":"2024-10-15T12:46:55.789332Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"  0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n  1%|          | 1/100 [00:03<06:14,  3.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  2%|▏         | 2/100 [00:26<24:42, 15.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  3%|▎         | 3/100 [01:05<41:35, 25.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  4%|▍         | 4/100 [01:59<59:11, 36.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  5%|▌         | 5/100 [02:19<48:56, 30.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  6%|▌         | 6/100 [03:10<58:55, 37.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  7%|▋         | 7/100 [03:23<45:48, 29.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  8%|▊         | 8/100 [03:57<47:41, 31.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n  9%|▉         | 9/100 [04:12<39:34, 26.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 10%|█         | 10/100 [04:27<33:56, 22.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 11%|█         | 11/100 [04:46<32:03, 21.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 12%|█▏        | 12/100 [05:15<35:00, 23.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 13%|█▎        | 13/100 [05:36<33:02, 22.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 14%|█▍        | 14/100 [05:42<25:28, 17.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 15%|█▌        | 15/100 [06:34<39:52, 28.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 16%|█▌        | 16/100 [06:44<31:36, 22.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 17%|█▋        | 17/100 [07:35<43:02, 31.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 18%|█▊        | 18/100 [08:21<48:51, 35.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 19%|█▉        | 19/100 [09:12<54:16, 40.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 20%|██        | 20/100 [09:15<38:49, 29.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 21%|██        | 21/100 [10:08<47:54, 36.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 22%|██▏       | 22/100 [10:28<40:34, 31.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 23%|██▎       | 23/100 [10:48<36:03, 28.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 24%|██▍       | 24/100 [11:18<36:14, 28.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 25%|██▌       | 25/100 [11:29<29:01, 23.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 26%|██▌       | 26/100 [11:50<27:47, 22.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 27%|██▋       | 27/100 [12:30<33:52, 27.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 28%|██▊       | 28/100 [12:38<26:23, 21.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 29%|██▉       | 29/100 [12:45<20:37, 17.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 30%|███       | 30/100 [13:34<31:16, 26.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 31%|███       | 31/100 [14:14<35:18, 30.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 32%|███▏      | 32/100 [14:43<34:14, 30.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 33%|███▎      | 33/100 [15:29<39:07, 35.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 34%|███▍      | 34/100 [16:21<44:08, 40.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 35%|███▌      | 35/100 [17:12<46:52, 43.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 36%|███▌      | 36/100 [17:25<36:41, 34.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 37%|███▋      | 37/100 [17:56<34:50, 33.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 38%|███▊      | 38/100 [18:15<30:06, 29.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 39%|███▉      | 39/100 [18:34<26:31, 26.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 40%|████      | 40/100 [18:47<22:06, 22.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 41%|████      | 41/100 [19:07<21:10, 21.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 42%|████▏     | 42/100 [19:18<17:46, 18.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 43%|████▎     | 43/100 [20:09<26:44, 28.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 44%|████▍     | 44/100 [20:33<25:07, 26.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 45%|████▌     | 45/100 [20:54<22:50, 24.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 46%|████▌     | 46/100 [21:49<30:43, 34.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 47%|████▋     | 47/100 [21:57<23:05, 26.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 48%|████▊     | 48/100 [22:18<21:16, 24.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 49%|████▉     | 49/100 [23:12<28:23, 33.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 50%|█████     | 50/100 [23:43<27:23, 32.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 51%|█████     | 51/100 [23:51<20:45, 25.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 52%|█████▏    | 52/100 [24:37<25:13, 31.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 53%|█████▎    | 53/100 [25:12<25:31, 32.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 54%|█████▍    | 54/100 [25:29<21:26, 27.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 55%|█████▌    | 55/100 [25:43<17:50, 23.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 56%|█████▌    | 56/100 [26:05<17:02, 23.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 57%|█████▋    | 57/100 [26:10<12:42, 17.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 58%|█████▊    | 58/100 [26:27<12:16, 17.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 59%|█████▉    | 59/100 [26:36<10:12, 14.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 60%|██████    | 60/100 [26:53<10:22, 15.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 61%|██████    | 61/100 [26:55<07:22, 11.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 62%|██████▏   | 62/100 [26:57<05:33,  8.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 63%|██████▎   | 63/100 [27:08<05:41,  9.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 64%|██████▍   | 64/100 [27:23<06:33, 10.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 65%|██████▌   | 65/100 [28:14<13:26, 23.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 66%|██████▌   | 66/100 [28:31<12:07, 21.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 67%|██████▋   | 67/100 [29:03<13:27, 24.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 68%|██████▊   | 68/100 [29:11<10:25, 19.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 69%|██████▉   | 69/100 [29:27<09:31, 18.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 70%|███████   | 70/100 [29:46<09:18, 18.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 71%|███████   | 71/100 [29:54<07:28, 15.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 72%|███████▏  | 72/100 [30:45<12:11, 26.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 73%|███████▎  | 73/100 [31:35<15:01, 33.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 74%|███████▍  | 74/100 [32:18<15:40, 36.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 75%|███████▌  | 75/100 [33:09<16:57, 40.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 76%|███████▌  | 76/100 [34:01<17:33, 43.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 77%|███████▋  | 77/100 [34:23<14:17, 37.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 78%|███████▊  | 78/100 [35:13<15:09, 41.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 79%|███████▉  | 79/100 [35:19<10:40, 30.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 80%|████████  | 80/100 [36:14<12:36, 37.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 81%|████████  | 81/100 [37:05<13:18, 42.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 82%|████████▏ | 82/100 [37:59<13:36, 45.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 83%|████████▎ | 83/100 [38:05<09:30, 33.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 84%|████████▍ | 84/100 [38:40<09:05, 34.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 85%|████████▌ | 85/100 [39:06<07:55, 31.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 86%|████████▌ | 86/100 [39:50<08:16, 35.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 87%|████████▋ | 87/100 [39:58<05:53, 27.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 88%|████████▊ | 88/100 [40:51<06:56, 34.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 89%|████████▉ | 89/100 [41:09<05:27, 29.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 90%|█████████ | 90/100 [41:33<04:40, 28.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 91%|█████████ | 91/100 [41:41<03:19, 22.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 92%|█████████▏| 92/100 [42:21<03:40, 27.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 93%|█████████▎| 93/100 [42:30<02:33, 21.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 94%|█████████▍| 94/100 [42:39<01:48, 18.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 95%|█████████▌| 95/100 [43:27<02:14, 27.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 96%|█████████▌| 96/100 [43:54<01:48, 27.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 97%|█████████▋| 97/100 [44:24<01:23, 27.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 98%|█████████▊| 98/100 [45:15<01:09, 34.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 99%|█████████▉| 99/100 [45:24<00:27, 27.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 100/100 [46:18<00:00, 27.78s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}